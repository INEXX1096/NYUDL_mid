# -*- coding: utf-8 -*-
"""DL_Mid

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17w_Y9W4OblVuhPbBrbJbNBO0JB6uy_h_

# Load Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

output_dir="/content/drive/MyDrive/llama3_8b_math_verifier_outputs"

"""# Install the Unsloth library and all its required dependencies."""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os, re, torch
# 
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     torch_version = torch.__version__.split("+")[0]
#     match = re.match(r"[0-9.]{3,}", torch_version)
#     v = match.group(0) if match else "unknown"
# 
#     if v.startswith("2.8.0"):
#         xformers = "xformers==0.0.32.post2"
#     else:
#         xformers = "xformers==0.0.29.post3"
# 
#     print(f"Detected torch version: {torch_version} → installing {xformers}")
# 
#     !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
#     !pip install --no-deps unsloth
# 
# !pip install transformers==4.56.2
# !pip install --no-deps trl==0.22.2
#

"""# Load the 4-bit quantized Llama 3.1 8B model and tokenizer using Unsloth."""

from unsloth import FastLanguageModel
import torch

max_seq_length = 1024  # Choose any sequence length
dtype = None  # This will auto-detect the best data type for your GPU
load_in_4bit = True  # Use 4-bit quantization to save memory

# Load the model and tokenizer from Hugging Face
# Note: We use the base model, not a 4-bit pre-quantized one,
# to ensure we start from the official weights.
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B", # Competition-approved model
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

"""# Load and split Dataset"""

from datasets import load_dataset

# Load the full training dataset
full_dataset = load_dataset("ad6398/nyu-dl-teach-maths-comp", split="train")

# Shuffle the dataset for randomness and create our smaller splits
shuffled_dataset = full_dataset.shuffle(seed=42)
train_dataset = shuffled_dataset.select(range(0, 10000))       # Use the first 5,000 for training
validation_dataset = shuffled_dataset.select(range(10000, 10500)) # Use the next 500 for validation

from datasets import load_dataset
import pandas as pd
from collections import Counter
import numpy as np
from tqdm import tqdm

# Load dataset
dataset = load_dataset("ad6398/nyu-dl-teach-maths-comp")

# Split
train_dataset = dataset['train']
test_dataset = dataset['test']

print("=== Dataset Overview ===")
print("Train samples:", len(train_dataset))
print("Test samples:", len(test_dataset))

# Convert to pandas for easier analysis
df_train = pd.DataFrame(train_dataset)

# Check columns
print("\nColumns:", df_train.columns.tolist())

# 1️⃣ Label distribution
label_counts = Counter(df_train['is_correct'])
total = sum(label_counts.values())
for k, v in label_counts.items():
    print(f"Label {k}: {v} samples ({v/total:.2%})")

# 2️⃣ Average token lengths (rough estimation by whitespace split)
df_train["q_len"] = df_train["question"].apply(lambda x: len(str(x).split()))
df_train["a_len"] = df_train["answer"].apply(lambda x: len(str(x).split()))
df_train["s_len"] = df_train["solution"].apply(lambda x: len(str(x).split()))

print("\n=== Average text lengths (by whitespace tokens) ===")
print(f"Question avg length: {df_train['q_len'].mean():.1f} tokens")
print(f"Answer avg length:   {df_train['a_len'].mean():.1f} tokens")
print(f"Solution avg length: {df_train['s_len'].mean():.1f} tokens")

# 3️⃣ Optional: question type breakdown (if it contains "geometry", "algebra", etc.)
def detect_type(text):
    text = text.lower()
    if any(k in text for k in ["triangle", "circle", "area", "angle"]):
        return "geometry"
    elif any(k in text for k in ["equation", "solve", "value", "expression"]):
        return "algebra"
    elif any(k in text for k in ["probability", "mean", "median", "percentage"]):
        return "statistics"
    elif any(k in text for k in ["number", "integer", "prime"]):
        return "arithmetic"
    else:
        return "other"

df_train["type"] = df_train["question"].apply(detect_type)
print("\n=== Question Type Breakdown ===")
print(df_train["type"].value_counts(normalize=True).round(3) * 100)

"""# Define the prompt template and apply it to format the training dataset."""

# The instructional prompt template for training
training_prompt = """You are a great mathematician and you are tasked with finding if a solution to a given maths question is correct or not. Your response should be 'True' if the solution is correct, otherwise 'False'. Below is the Question and Solution.
Question:
{}
Solution:
{}
Output:
"""

# We must add an End Of Sequence (EOS) token to tell the model when a completion is finished.
EOS_TOKEN = tokenizer.eos_token

# This function formats our data samples into the prompt template.
def formatting_prompts_func(examples):
    questions = examples["question"]
    solutions = examples["solution"]
    outputs = examples["is_correct"]
    texts = []
    for question, solution, output in zip(questions, solutions, outputs):
        # Format the prompt and add the EOS token
        text = training_prompt.format(question, str(solution), str(output)) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts }

# Apply the formatting function to our training dataset
formatted_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # A small rank for lighter training
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 32, # A common practice is to set alpha = 2 * r
    lora_dropout = 0.03,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 42,
)

"""# Generate validation Dataset"""

formatted_validation_dataset = validation_dataset.map(formatting_prompts_func, batched=True)
formatted_validation_dataset = formatted_validation_dataset.select(range(50))

"""# Adjust the trainer"""

from sklearn.metrics import accuracy_score
from transformers import TrainingArguments
from trl import SFTTrainer


trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=formatted_train_dataset,
    eval_dataset=formatted_validation_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    args=TrainingArguments(

        per_device_train_batch_size=32,
        gradient_accumulation_steps=8,
        warmup_steps=20,
        max_steps=1100,

        learning_rate=2e-5,
        lr_scheduler_type="cosine",
        weight_decay=0.01,

        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        optim="adamw_8bit",
        gradient_checkpointing=True,

        eval_strategy="steps",
        eval_steps=50,
        save_strategy="steps",
        save_steps=50,
        load_best_model_at_end=True,
        output_dir= output_dir,

        logging_steps=20,
        report_to="none",
        seed=42,
        resume_from_checkpoint=checkpoint_path,
    ),
)

"""# Define Output path"""

checkpoint_path = "/content/drive/MyDrive/llama3_8b_math_verifier_outputs/checkpoint-900"

"""# Train From checkpoint"""

trainer.train(resume_from_checkpoint=checkpoint_path)

"""# Generate Submission File"""

import pandas as pd
from tqdm import tqdm

# Load the official test set
test_dataset = load_dataset("ad6398/nyu-dl-teach-maths-comp", split="test")
predictions = []

# A simple function to parse 'True' or 'False' from the model's raw output
def parse_output(response_text):
    # Find the text after "Output:"
    output_part = response_text.split("Output:\n")[-1]
    # Check if "True" is in that part, case-insensitively
    if 'true' in output_part.lower():
        return True
    return False

# Loop through the test dataset and generate a prediction for each example
for example in tqdm(test_dataset):
    question = example["question"]
    solution = example["solution"]

    # Format the prompt
    prompt = inference_prompt.format(question, str(solution))
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

    # Generate the prediction
    outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)
    response_text = tokenizer.batch_decode(outputs)[0]

    # Parse the prediction and add it to our list
    prediction = parse_output(response_text)
    predictions.append(prediction)

# Create the submission DataFrame
submission = pd.DataFrame({
    'ID': range(len(predictions)),
    'is_correct': predictions
})

# Save the DataFrame to a CSV file
submission.to_csv('submission.csv', index=False)

print("\nSubmission file 'submission.csv' created successfully!")
print("You can now download this file and submit it to the Kaggle competition.")